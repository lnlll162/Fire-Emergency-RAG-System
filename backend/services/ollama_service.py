#!/usr/bin/env python3
"""
OllamaæœåŠ¡
æä¾›åŸºäºOllamaçš„AIç”ŸæˆåŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•‘æ´æ–¹æ¡ˆç”Ÿæˆã€æ¨¡å‹çŠ¶æ€ç®¡ç†ç­‰
"""

import asyncio
import logging
import sys
import json
import hashlib
import os
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timedelta
from pathlib import Path

# è®¾ç½®ä»£ç†è·³è¿‡localhostï¼ˆè§£å†³Windowsä»£ç†å¯¼è‡´çš„502é”™è¯¯ï¼‰
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'
os.environ['no_proxy'] = 'localhost,127.0.0.1,::1'

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from pydantic import BaseModel, Field
import httpx
import redis
from redis.exceptions import RedisError

from shared.config import get_config
from shared.exceptions import ExternalServiceError, TimeoutError, ServiceError
from shared.models import RescuePlan, RescuePlanRequest, Item, Environment, RescueStep, PriorityLevel

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# æ•°æ®æ¨¡å‹
class ModelInfo(BaseModel):
    """æ¨¡å‹ä¿¡æ¯"""
    name: str = Field(..., description="æ¨¡å‹åç§°")
    size: Optional[int] = Field(None, description="æ¨¡å‹å¤§å°(å­—èŠ‚)")
    digest: Optional[str] = Field(None, description="æ¨¡å‹æ‘˜è¦")
    modified_at: Optional[datetime] = Field(None, description="ä¿®æ”¹æ—¶é—´")
    family: Optional[str] = Field(None, description="æ¨¡å‹ç³»åˆ—")
    format: Optional[str] = Field(None, description="æ¨¡å‹æ ¼å¼")
    families: Optional[List[str]] = Field(None, description="æ¨¡å‹ç³»åˆ—åˆ—è¡¨")
    parameter_size: Optional[str] = Field(None, description="å‚æ•°é‡")
    quantization_level: Optional[str] = Field(None, description="é‡åŒ–çº§åˆ«")

class ModelStatus(BaseModel):
    """æ¨¡å‹çŠ¶æ€"""
    name: str = Field(..., description="æ¨¡å‹åç§°")
    loaded: bool = Field(..., description="æ˜¯å¦å·²åŠ è½½")
    loading: bool = Field(default=False, description="æ˜¯å¦æ­£åœ¨åŠ è½½")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")
    last_used: Optional[datetime] = Field(None, description="æœ€åä½¿ç”¨æ—¶é—´")
    memory_usage: Optional[int] = Field(None, description="å†…å­˜ä½¿ç”¨é‡(å­—èŠ‚)")

class GenerationRequest(BaseModel):
    """ç”Ÿæˆè¯·æ±‚"""
    prompt: str = Field(..., description="æç¤ºè¯", min_length=1, max_length=10000)
    model: Optional[str] = Field(None, description="æŒ‡å®šæ¨¡å‹åç§°")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="Top-på‚æ•°")
    max_tokens: int = Field(default=2048, ge=1, le=8192, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¾“å‡º")
    context: Optional[Dict[str, Any]] = Field(None, description="ä¸Šä¸‹æ–‡ä¿¡æ¯")

class GenerationResponse(BaseModel):
    """ç”Ÿæˆå“åº”"""
    response: str = Field(..., description="ç”Ÿæˆçš„æ–‡æœ¬")
    model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹")
    created_at: datetime = Field(default_factory=datetime.now, description="åˆ›å»ºæ—¶é—´")
    tokens_used: Optional[int] = Field(None, description="ä½¿ç”¨çš„tokenæ•°")
    generation_time: float = Field(..., description="ç”Ÿæˆè€—æ—¶(ç§’)")
    cached: bool = Field(default=False, description="æ˜¯å¦æ¥è‡ªç¼“å­˜")

class RescuePlanGenerationRequest(BaseModel):
    """æ•‘æ´æ–¹æ¡ˆç”Ÿæˆè¯·æ±‚"""
    items: List[Item] = Field(..., description="ç‰©å“åˆ—è¡¨", min_items=1, max_items=50)
    environment: Environment = Field(..., description="ç¯å¢ƒä¿¡æ¯")
    additional_info: Optional[str] = Field(None, description="é™„åŠ ä¿¡æ¯", max_length=1000)
    urgency_level: str = Field(default="ä¸­", description="ç´§æ€¥ç¨‹åº¦")
    model: Optional[str] = Field(None, description="æŒ‡å®šæ¨¡å‹åç§°")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")

class QueryResponse(BaseModel):
    """æŸ¥è¯¢å“åº”"""
    success: bool = Field(..., description="æŸ¥è¯¢æ˜¯å¦æˆåŠŸ")
    data: Any = Field(..., description="æŸ¥è¯¢ç»“æœæ•°æ®")
    message: str = Field(default="", description="å“åº”æ¶ˆæ¯")
    timestamp: datetime = Field(default_factory=datetime.now, description="å“åº”æ—¶é—´")

class OllamaService:
    """OllamaæœåŠ¡ç±»"""
    
    def __init__(self):
        self.config = get_config()
        self.ollama_url = self.config.database.ollama_url
        self.redis_client = None
        self.current_model = self.config.database.ollama_model  # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        self.model_cache = {}
        self._initialize_redis()
        self._initialize_ollama_connection()
    
    def _initialize_redis(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        try:
            self.redis_client = redis.Redis(
                host=self.config.database.redis_host,
                port=self.config.database.redis_port,
                password=self.config.database.redis_password,
                db=self.config.database.redis_db,
                decode_responses=True
            )
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            logger.info("Redisè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"Redisè¿æ¥åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.redis_client = None
    
    def _initialize_ollama_connection(self):
        """åˆå§‹åŒ–Ollamaè¿æ¥"""
        try:
            # æµ‹è¯•Ollamaè¿æ¥
            asyncio.run(self._test_ollama_connection())
            logger.info("Ollamaè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"Ollamaè¿æ¥åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise ExternalServiceError(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {str(e)}", "ollama")
    
    async def _test_ollama_connection(self) -> bool:
        """æµ‹è¯•Ollamaè¿æ¥"""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                return response.status_code == 200
        except Exception as e:
            logger.error(f"Ollamaè¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
    
    async def test_connection(self) -> bool:
        """æµ‹è¯•æœåŠ¡è¿æ¥"""
        try:
            # æµ‹è¯•Ollamaè¿æ¥
            ollama_ok = await self._test_ollama_connection()
            
            # æµ‹è¯•Redisè¿æ¥
            redis_ok = False
            if self.redis_client:
                try:
                    self.redis_client.ping()
                    redis_ok = True
                except:
                    pass
            
            return ollama_ok and redis_ok
        except Exception as e:
            logger.error(f"æœåŠ¡è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
    
    async def list_models(self) -> List[ModelInfo]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                response.raise_for_status()
                
                data = response.json()
                models = []
                
                for model_data in data.get("models", []):
                    model_info = ModelInfo(
                        name=model_data.get("name", ""),
                        size=model_data.get("size"),
                        digest=model_data.get("digest"),
                        modified_at=datetime.fromisoformat(model_data.get("modified_at", "").replace("Z", "+00:00")) if model_data.get("modified_at") else None,
                        family=model_data.get("family"),
                        format=model_data.get("format"),
                        families=model_data.get("families"),
                        parameter_size=model_data.get("parameter_size"),
                        quantization_level=model_data.get("quantization_level")
                    )
                    models.append(model_info)
                
                return models
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
            raise ExternalServiceError(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}", "ollama")
    
    async def get_model_status(self, model_name: str) -> ModelStatus:
        """è·å–æ¨¡å‹çŠ¶æ€"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨è¿è¡Œ
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/show",
                    json={"name": model_name}
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return ModelStatus(
                        name=model_name,
                        loaded=True,
                        loading=False,
                        memory_usage=data.get("size"),
                        last_used=datetime.now()
                    )
                else:
                    return ModelStatus(
                        name=model_name,
                        loaded=False,
                        loading=False,
                        error=f"æ¨¡å‹çŠ¶æ€æ£€æŸ¥å¤±è´¥: {response.status_code}"
                    )
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹çŠ¶æ€å¤±è´¥: {str(e)}")
            return ModelStatus(
                name=model_name,
                loaded=False,
                loading=False,
                error=str(e)
            )
    
    async def load_model(self, model_name: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": model_name,
                        "prompt": "æµ‹è¯•",
                        "stream": False
                    }
                )
                
                if response.status_code == 200:
                    self.current_model = model_name
                    logger.info(f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
                    return True
                else:
                    logger.error(f"æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {response.status_code}")
                    return False
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            return False
    
    def _generate_cache_key(self, request: GenerationRequest) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºè¯·æ±‚çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®
        request_data = {
            "prompt": request.prompt,
            "model": request.model or self.current_model,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "max_tokens": request.max_tokens
        }
        request_str = json.dumps(request_data, sort_keys=True)
        return f"ollama:generation:{hashlib.md5(request_str.encode()).hexdigest()}"
    
    async def generate_text(self, request: GenerationRequest) -> GenerationResponse:
        """ç”Ÿæˆæ–‡æœ¬"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if self.redis_client:
                cache_key = self._generate_cache_key(request)
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    data = json.loads(cached_result)
                    logger.info(f"ä»ç¼“å­˜è¿”å›ç”Ÿæˆç»“æœ: {cache_key}")
                    # ç§»é™¤cachedå­—æ®µé¿å…é‡å¤
                    data.pop('cached', None)
                    return GenerationResponse(**data, cached=True)
            
            # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–å½“å‰æ¨¡å‹
            model_name = request.model or self.current_model
            if not model_name:
                raise ValueError("æœªæŒ‡å®šæ¨¡å‹ä¸”å½“å‰æ— å¯ç”¨æ¨¡å‹")
            
            start_time = datetime.now()
            
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": model_name,
                        "prompt": request.prompt,
                        "stream": request.stream,
                        "options": {
                            "temperature": request.temperature,
                            "top_p": request.top_p,
                            "num_predict": request.max_tokens
                        }
                    }
                )
                
                response.raise_for_status()
                data = response.json()
                
                end_time = datetime.now()
                generation_time = (end_time - start_time).total_seconds()
                
                result = GenerationResponse(
                    response=data.get("response", ""),
                    model=model_name,
                    tokens_used=data.get("eval_count"),
                    generation_time=generation_time
                )
                
                # ç¼“å­˜ç»“æœï¼ˆç¼“å­˜1å°æ—¶ï¼‰
                if self.redis_client:
                    cache_key = self._generate_cache_key(request)
                    cache_data = result.model_dump()
                    cache_data["cached"] = False
                    self.redis_client.setex(
                        cache_key,
                        3600,  # 1å°æ—¶
                        json.dumps(cache_data, default=str)
                    )
                
                return result
                
        except httpx.TimeoutException as e:
            raise TimeoutError(f"ç”Ÿæˆè¯·æ±‚è¶…æ—¶: {str(e)}", "ollama", 120.0)
        except httpx.HTTPError as e:
            raise ExternalServiceError(f"ç”Ÿæˆè¯·æ±‚å¤±è´¥: {str(e)}", "ollama")
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥: {str(e)}")
            raise ServiceError(f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥: {str(e)}", "ollama_service")
    
    def _build_rescue_plan_prompt(self, request: RescuePlanGenerationRequest) -> str:
        """æ„å»ºæ•‘æ´æ–¹æ¡ˆç”Ÿæˆæç¤ºè¯"""
        # æ„å»ºç‰©å“ä¿¡æ¯
        items_info = []
        for item in request.items:
            item_desc = f"- {item.name} ({item.material.value})"
            if item.quantity > 1:
                item_desc += f" x{item.quantity}"
            if item.location:
                item_desc += f" ä½ç½®: {item.location}"
            if item.condition:
                item_desc += f" çŠ¶æ€: {item.condition}"
            items_info.append(item_desc)
        
        # æ„å»ºç¯å¢ƒä¿¡æ¯
        env_info = f"""
ç¯å¢ƒç±»å‹: {request.environment.type.value}
åŒºåŸŸç±»å‹: {request.environment.area.value}
é€šé£æƒ…å†µ: {request.environment.ventilation.value}
å‡ºå£æ•°é‡: {request.environment.exits}
"""
        if request.environment.floor is not None:
            env_info += f"æ¥¼å±‚: {request.environment.floor}\n"
        if request.environment.occupancy is not None:
            env_info += f"äººå‘˜æ•°é‡: {request.environment.occupancy}\n"
        if request.environment.building_type:
            env_info += f"å»ºç­‘ç±»å‹: {request.environment.building_type}\n"
        if request.environment.special_conditions:
            env_info += f"ç‰¹æ®Šæ¡ä»¶: {request.environment.special_conditions}\n"
        
        # æ„å»ºå®Œæ•´æç¤ºè¯
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç«ç¾åº”æ€¥æ•‘æ´ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè¯¦ç»†çš„æ•‘æ´æ–¹æ¡ˆã€‚

## åœºæ™¯ä¿¡æ¯

### ç‰©å“ä¿¡æ¯
{chr(10).join(items_info)}

### ç¯å¢ƒä¿¡æ¯
{env_info}

### ç´§æ€¥ç¨‹åº¦
{request.urgency_level}

### é™„åŠ ä¿¡æ¯
{request.additional_info or "æ— "}

## è¾“å‡ºè¦æ±‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆæ•‘æ´æ–¹æ¡ˆï¼Œå¿…é¡»åŒ…å«æ¯ä¸ªæ­¥éª¤çš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼š

### ç¬¬ä¸€æ­¥ï¼šç«‹å³å“åº”
- æè¿°ï¼šè¿…é€Ÿæ‹¨æ‰“119æŠ¥è­¦ï¼Œç»„ç»‡ç°åœºäººå‘˜æœ‰åºæ’¤ç¦»ï¼Œå…³é—­ç”µæºå’Œç‡ƒæ°”é˜€é—¨ï¼Œé¿å…ç«åŠ¿è”“å»¶
- æ‰€éœ€è®¾å¤‡ï¼šæ‰‹æœºï¼Œåº”æ€¥ç…§æ˜ç¯ï¼Œæ‰©éŸ³å™¨ï¼Œé˜²çƒŸé¢ç½©
- æ³¨æ„äº‹é¡¹ï¼šç¡®ä¿è‡ªèº«å®‰å…¨ï¼Œä¿æŒå†·é™ï¼Œä¼˜å…ˆç–æ•£è€äººå’Œå„¿ç«¥ï¼Œä¸è¦ä½¿ç”¨ç”µæ¢¯
- é¢„è®¡æ—¶é—´ï¼š3åˆ†é’Ÿ

### ç¬¬äºŒæ­¥ï¼šç«åŠ¿æ§åˆ¶
- æè¿°ï¼šä½¿ç”¨é€‚å½“çš„ç­ç«å™¨æå¯¹ç«æºè¿›è¡ŒåˆæœŸæ‰‘æ•‘ï¼Œåˆ‡æ–­ç«åŠ¿è”“å»¶è·¯å¾„ï¼Œé‡ç‚¹ä¿æŠ¤é‡è¦è®¾æ–½
- æ‰€éœ€è®¾å¤‡ï¼šå¹²ç²‰ç­ç«å™¨ï¼Œæ¶ˆé˜²æ°´å¸¦ï¼Œé˜²ç«æ¯¯ï¼Œé˜²æŠ¤æœï¼Œå‘¼å¸å™¨
- æ³¨æ„äº‹é¡¹ï¼šæ³¨æ„é£å‘ï¼Œç«™åœ¨ä¸Šé£ä½ç½®ï¼Œä¿æŒå®‰å…¨è·ç¦»ï¼Œä¸è¦è´¸ç„¶è¿›å…¥æµ“çƒŸåŒºåŸŸ
- é¢„è®¡æ—¶é—´ï¼š10åˆ†é’Ÿ

### ç¬¬ä¸‰æ­¥ï¼šäººå‘˜ç–æ•£
- æè¿°ï¼šå¼•å¯¼æ‰€æœ‰äººå‘˜æŒ‰ç…§ç–æ•£æŒ‡ç¤ºæ ‡å¿—æ’¤ç¦»ï¼Œæ¸…ç‚¹äººæ•°ï¼Œç¡®è®¤æ— äººå‘˜æ»ç•™
- æ‰€éœ€è®¾å¤‡ï¼šåº”æ€¥ç¯ï¼Œå¯¹è®²æœºï¼Œæ€¥æ•‘ç®±ï¼Œè­¦æˆ’å¸¦
- æ³¨æ„äº‹é¡¹ï¼šä½å§¿å‰è¿›ï¼Œç”¨æ¹¿æ¯›å·¾æ‚ä½å£é¼»ï¼Œä¸è¦è¿”å›ç«åœºï¼Œåˆ°è¾¾å®‰å…¨åŒºåŸŸåç«‹å³æ¸…ç‚¹äººæ•°
- é¢„è®¡æ—¶é—´ï¼š5åˆ†é’Ÿ

### ç¬¬å››æ­¥ï¼šç°åœºæ¸…ç†
- æè¿°ï¼šç¡®è®¤ç«åŠ¿å®Œå…¨æ‰‘ç­ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¤ç‡ƒé£é™©ï¼Œæ¸…ç†ç°åœºæ®‹ç•™å±é™©ç‰©å“ï¼Œè¯„ä¼°æŸå¤±
- æ‰€éœ€è®¾å¤‡ï¼šçƒ­æˆåƒä»ªï¼Œæ¸…ç†å·¥å…·ï¼Œè­¦æˆ’æ ‡è¯†ï¼Œç›¸æœº
- æ³¨æ„äº‹é¡¹ï¼šç­‰å¾…æ¶ˆé˜²é˜Ÿç¡®è®¤å®‰å…¨ï¼Œæ³¨æ„å»ºç­‘ç»“æ„ç¨³å®šæ€§ï¼Œåšå¥½ç°åœºä¿æŠ¤å·¥ä½œ
- é¢„è®¡æ—¶é—´ï¼š15åˆ†é’Ÿ

é‡è¦ï¼šä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼ç”Ÿæˆ4ä¸ªå®Œæ•´æ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤å¿…é¡»åŒ…å«ï¼š
1. æè¿°ï¼šå…·ä½“ã€å¯æ“ä½œçš„è¡ŒåŠ¨è¯´æ˜ï¼ˆè‡³å°‘15ä¸ªå­—ï¼‰
2. æ‰€éœ€è®¾å¤‡ï¼šè¯¦ç»†çš„è®¾å¤‡æ¸…å•ï¼ˆè‡³å°‘åˆ—å‡º3-5é¡¹ï¼‰
3. æ³¨æ„äº‹é¡¹ï¼šå®‰å…¨è¦ç‚¹å’Œæ³¨æ„äº‹é¡¹ï¼ˆè‡³å°‘åˆ—å‡º2-3æ¡ï¼‰
4. é¢„è®¡æ—¶é—´ï¼šåˆç†çš„æ—¶é—´ä¼°è®¡ï¼ˆ3-20åˆ†é’Ÿï¼‰

ç°åœ¨è¯·ç”Ÿæˆæ•‘æ´æ–¹æ¡ˆï¼š
"""
        return prompt
    
    def _parse_rescue_plan_response(self, response: str, urgency_level: str = "ä¸­") -> RescuePlan:
        """è§£ææ•‘æ´æ–¹æ¡ˆå“åº”"""
        try:
            # æ¸…ç†å“åº”æ–‡æœ¬
            response = response.strip()
            lines = [line.strip() for line in response.split('\n') if line.strip()]
            
            # æå–æ ‡é¢˜ï¼ˆæŸ¥æ‰¾"#"æˆ–"æ ‡é¢˜"å…³é”®è¯ï¼‰
            title = "ç«ç¾åº”æ€¥æ•‘æ´æ–¹æ¡ˆ"
            for line in lines:
                if line.startswith("#") and ("æ–¹æ¡ˆ" in line or "æ ‡é¢˜" in line):
                    title = line.replace("#", "").strip()
                    break
                elif "æ ‡é¢˜" in line and ":" in line:
                    title = line.split(":", 1)[1].strip()
                    break
            
            # æå–æ­¥éª¤ - ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•
            steps = []
            current_step = None
            step_number = 1
            current_section = None
            
            import re
            
            for i, line in enumerate(lines):
                # æ£€æµ‹æ–°æ­¥éª¤ï¼ˆåŒ¹é… "### ç¬¬Xæ­¥ï¼š" æ ¼å¼ï¼‰
                step_match = re.match(r'^#{1,3}\s*ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)æ­¥[ï¼š:](.+)', line)
                
                if step_match:
                    # ä¿å­˜å‰ä¸€ä¸ªæ­¥éª¤
                    if current_step:
                        steps.append(current_step)
                    
                    # æå–æ­¥éª¤æ ‡é¢˜ä½œä¸ºæè¿°
                    step_title = step_match.group(2).strip()
                    
                    current_step = RescueStep(
                        step_number=step_number,
                        description=step_title,
                        equipment=[],
                        warnings=[],
                        estimated_time=5
                    )
                    step_number += 1
                    current_section = None
                
                # å¦‚æœåœ¨æ­¥éª¤å†…ï¼Œè§£æå„ä¸ªå­—æ®µï¼ˆæ”¯æŒMarkdownåŠ ç²—è¯­æ³• **ï¼‰
                elif current_step:
                    # æ£€æµ‹æè¿°ï¼ˆæ”¯æŒ **æè¿°** æˆ– æè¿°ï¼‰
                    desc_match = re.match(r'^-?\s*\**(æè¿°|è¯´æ˜)\**[ï¼š:](.+)', line)
                    if desc_match:
                        desc_text = desc_match.group(2).strip()
                        if desc_text and desc_text != "[å…·ä½“æ“ä½œæè¿°]":
                            current_step.description = desc_text
                        current_section = 'description'
                    
                    # æ£€æµ‹è®¾å¤‡ï¼ˆæ”¯æŒ **æ‰€éœ€è®¾å¤‡** æˆ– æ‰€éœ€è®¾å¤‡ï¼‰
                    elif re.match(r'^-?\s*\**æ‰€éœ€è®¾å¤‡\**[ï¼š:]', line):
                        current_section = 'equipment'
                        equipment_text = re.sub(r'^-?\s*\**æ‰€éœ€è®¾å¤‡\**[ï¼š:]', '', line).strip()
                        # ç§»é™¤å¯èƒ½çš„å¥å·
                        equipment_text = equipment_text.rstrip('ã€‚.')
                        if equipment_text and equipment_text != "[è®¾å¤‡åˆ—è¡¨]":
                            equipment_items = [eq.strip() for eq in re.split(r'[,ï¼Œã€]', equipment_text) if eq.strip()]
                            current_step.equipment.extend(equipment_items)
                    
                    # æ£€æµ‹æ³¨æ„äº‹é¡¹ï¼ˆæ”¯æŒ **æ³¨æ„äº‹é¡¹** æˆ– æ³¨æ„äº‹é¡¹ï¼‰
                    elif re.match(r'^-?\s*\**æ³¨æ„äº‹é¡¹\**[ï¼š:]', line):
                        current_section = 'warnings'
                        warning_text = re.sub(r'^-?\s*\**æ³¨æ„äº‹é¡¹\**[ï¼š:]', '', line).strip()
                        # ç§»é™¤å¯èƒ½çš„å¥å·
                        warning_text = warning_text.rstrip('ã€‚.')
                        if warning_text and warning_text != "[å®‰å…¨æé†’]":
                            current_step.warnings.append(warning_text)
                    
                    # æ£€æµ‹é¢„è®¡æ—¶é—´ï¼ˆæ”¯æŒ **é¢„è®¡æ—¶é—´** æˆ– é¢„è®¡æ—¶é—´ï¼‰
                    elif re.match(r'^-?\s*\**é¢„è®¡æ—¶é—´\**[ï¼š:]', line):
                        current_section = 'time'
                        time_text = re.sub(r'^-?\s*\**é¢„è®¡æ—¶é—´\**[ï¼š:]', '', line).strip()
                        # ç§»é™¤å¯èƒ½çš„å¥å·
                        time_text = time_text.rstrip('ã€‚.')
                        time_match = re.search(r'(\d+)\s*åˆ†é’Ÿ', time_text)
                        if time_match:
                            current_step.estimated_time = int(time_match.group(1))
                        else:
                            # å°è¯•åŒ¹é…çº¯æ•°å­—
                            num_match = re.search(r'(\d+)', time_text)
                            if num_match:
                                current_step.estimated_time = int(num_match.group(1))
                    
                    # ç»§ç»­æ”¶é›†åˆ—è¡¨é¡¹
                    elif line.startswith("-") or line.startswith("â€¢"):
                        item_text = line.lstrip("-â€¢").strip()
                        if current_section == 'equipment' and item_text:
                            current_step.equipment.append(item_text)
                        elif current_section == 'warnings' and item_text:
                            current_step.warnings.append(item_text)
            
            # æ·»åŠ æœ€åä¸€ä¸ªæ­¥éª¤
            if current_step:
                steps.append(current_step)
            
            # å¦‚æœæ²¡æœ‰è§£æåˆ°æ­¥éª¤ï¼Œåˆ›å»ºé»˜è®¤æ­¥éª¤
            if not steps:
                steps = [
                    RescueStep(
                        step_number=1,
                        description="è¯„ä¼°ç°åœºæƒ…å†µï¼Œç¡®ä¿æ•‘æ´äººå‘˜å®‰å…¨",
                        equipment=["é˜²æŠ¤è£…å¤‡", "é€šä¿¡è®¾å¤‡"],
                        warnings=["æ³¨æ„ç°åœºå®‰å…¨", "ä¿æŒé€šä¿¡ç•…é€š"],
                        estimated_time=5
                    ),
                    RescueStep(
                        step_number=2,
                        description="åˆ¶å®šå…·ä½“æ•‘æ´æ–¹æ¡ˆå¹¶æ‰§è¡Œ",
                        equipment=["æ•‘æ´å·¥å…·", "å®‰å…¨è®¾å¤‡"],
                        warnings=["ä¸¥æ ¼æŒ‰ç…§å®‰å…¨ç¨‹åºæ“ä½œ"],
                        estimated_time=30
                    )
                ]
            
            # æå–è®¾å¤‡æ¸…å•
            equipment_list = []
            for step in steps:
                equipment_list.extend(step.equipment)
            equipment_list = list(set(equipment_list))  # å»é‡
            
            # æå–è­¦å‘Š
            warnings = []
            for step in steps:
                warnings.extend(step.warnings)
            warnings = list(set(warnings))  # å»é‡
            
            # è®¡ç®—æ€»æ—¶é•¿
            total_duration = sum(step.estimated_time or 0 for step in steps)
            if total_duration == 0:
                total_duration = 60  # é»˜è®¤1å°æ—¶
            
            # ç¡®å®šä¼˜å…ˆçº§
            priority = PriorityLevel.HIGH
            if urgency_level in ["ä½", "ä¸€èˆ¬"]:
                priority = PriorityLevel.MEDIUM
            elif urgency_level in ["ç´§æ€¥", "éå¸¸ç´§æ€¥"]:
                priority = PriorityLevel.URGENT
            
            return RescuePlan(
                title=title,
                priority=priority,
                steps=steps,
                equipment_list=equipment_list,
                warnings=warnings,
                estimated_duration=total_duration
            )
            
        except Exception as e:
            logger.error(f"è§£ææ•‘æ´æ–¹æ¡ˆå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤æ–¹æ¡ˆ
            return RescuePlan(
                title="ç«ç¾åº”æ€¥æ•‘æ´æ–¹æ¡ˆ",
                priority=PriorityLevel.HIGH,
                steps=[
                    RescueStep(
                        step_number=1,
                        description="è¯„ä¼°ç°åœºæƒ…å†µï¼Œç¡®ä¿æ•‘æ´äººå‘˜å®‰å…¨",
                        equipment=["é˜²æŠ¤è£…å¤‡", "é€šä¿¡è®¾å¤‡"],
                        warnings=["æ³¨æ„ç°åœºå®‰å…¨", "ä¿æŒé€šä¿¡ç•…é€š"],
                        estimated_time=5
                    ),
                    RescueStep(
                        step_number=2,
                        description="åˆ¶å®šå…·ä½“æ•‘æ´æ–¹æ¡ˆå¹¶æ‰§è¡Œ",
                        equipment=["æ•‘æ´å·¥å…·", "å®‰å…¨è®¾å¤‡"],
                        warnings=["ä¸¥æ ¼æŒ‰ç…§å®‰å…¨ç¨‹åºæ“ä½œ"],
                        estimated_time=30
                    )
                ],
                equipment_list=["é˜²æŠ¤è£…å¤‡", "é€šä¿¡è®¾å¤‡", "æ•‘æ´å·¥å…·", "å®‰å…¨è®¾å¤‡"],
                warnings=["æ³¨æ„ç°åœºå®‰å…¨", "ä¿æŒé€šä¿¡ç•…é€š", "ä¸¥æ ¼æŒ‰ç…§å®‰å…¨ç¨‹åºæ“ä½œ"],
                estimated_duration=60
            )
    
    async def generate_rescue_plan(self, request: RescuePlanGenerationRequest) -> RescuePlan:
        """ç”Ÿæˆæ•‘æ´æ–¹æ¡ˆ"""
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_rescue_plan_prompt(request)
            
            # ç”Ÿæˆæ–‡æœ¬
            generation_request = GenerationRequest(
                prompt=prompt,
                model=request.model,
                temperature=request.temperature,
                max_tokens=2048
            )
            
            response = await self.generate_text(generation_request)
            
            # æ‰“å°åŸå§‹å“åº”ç”¨äºè°ƒè¯•
            logger.info(f"ğŸ” OllamaåŸå§‹å“åº”ï¼ˆå‰500å­—ç¬¦ï¼‰: {response.response[:500]}")
            
            # è§£ææ•‘æ´æ–¹æ¡ˆ
            rescue_plan = self._parse_rescue_plan_response(response.response, request.urgency_level)
            
            # æ‰“å°è§£æç»“æœ
            logger.info(f"âœ… æ•‘æ´æ–¹æ¡ˆç”ŸæˆæˆåŠŸï¼ŒåŒ…å« {len(rescue_plan.steps)} ä¸ªæ­¥éª¤")
            for i, step in enumerate(rescue_plan.steps, 1):
                logger.info(f"   æ­¥éª¤{i}: {step.description[:50]}... | è®¾å¤‡æ•°: {len(step.equipment)} | æ³¨æ„äº‹é¡¹æ•°: {len(step.warnings)}")
            return rescue_plan
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ•‘æ´æ–¹æ¡ˆå¤±è´¥: {str(e)}")
            raise ServiceError(f"ç”Ÿæˆæ•‘æ´æ–¹æ¡ˆå¤±è´¥: {str(e)}", "ollama_service")
    
    async def clear_cache(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        try:
            if self.redis_client:
                # åˆ é™¤æ‰€æœ‰ollamaç›¸å…³çš„ç¼“å­˜
                keys = self.redis_client.keys("ollama:*")
                if keys:
                    self.redis_client.delete(*keys)
                logger.info(f"æ¸…ç©ºäº† {len(keys)} ä¸ªç¼“å­˜é¡¹")
                return True
            return False
        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}")
            return False
    
    async def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not self.redis_client:
                return {"enabled": False, "message": "Redisæœªè¿æ¥"}
            
            # è·å–ollamaç›¸å…³çš„ç¼“å­˜é”®
            keys = self.redis_client.keys("ollama:*")
            total_keys = len(keys)
            
            # è®¡ç®—æ€»å†…å­˜ä½¿ç”¨
            total_memory = 0
            for key in keys:
                try:
                    memory = self.redis_client.memory_usage(key)
                    total_memory += memory or 0
                except:
                    pass
            
            return {
                "enabled": True,
                "total_keys": total_keys,
                "total_memory_bytes": total_memory,
                "total_memory_mb": round(total_memory / 1024 / 1024, 2)
            }
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {"enabled": False, "error": str(e)}

# åˆ›å»ºæœåŠ¡å®ä¾‹
ollama_service = OllamaService()

# FastAPIåº”ç”¨
app = FastAPI(
    title="OllamaæœåŠ¡",
    description="åŸºäºOllamaçš„AIç”ŸæˆæœåŠ¡ï¼Œæä¾›æ•‘æ´æ–¹æ¡ˆç”Ÿæˆå’Œæ¨¡å‹ç®¡ç†åŠŸèƒ½",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# ä¾èµ–æ³¨å…¥
def get_ollama_service() -> OllamaService:
    return ollama_service

# APIè·¯ç”±
@app.get("/", response_model=QueryResponse)
async def root():
    """æ ¹è·¯å¾„ - æœåŠ¡ä¿¡æ¯"""
    return QueryResponse(
        success=True,
        data={
            "service": "OllamaæœåŠ¡",
            "version": "1.0.0",
            "description": "åŸºäºOllamaçš„AIç”ŸæˆæœåŠ¡",
            "endpoints": {
                "health": "/health",
                "models": "/models",
                "generate": "/generate",
                "rescue-plan": "/rescue-plan",
                "cache": "/cache",
                "docs": "/docs"
            }
        },
        message="OllamaæœåŠ¡è¿è¡Œæ­£å¸¸"
    )

@app.get("/health", response_model=QueryResponse)
async def health_check(service: OllamaService = Depends(get_ollama_service)):
    """å¥åº·æ£€æŸ¥"""
    try:
        is_healthy = await service.test_connection()
        return QueryResponse(
            success=is_healthy,
            data={"status": "healthy" if is_healthy else "unhealthy"},
            message="æœåŠ¡å¥åº·æ£€æŸ¥å®Œæˆ"
        )
    except Exception as e:
        return QueryResponse(
            success=False,
            data={"status": "unhealthy"},
            message=f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}"
        )

@app.get("/models", response_model=QueryResponse)
async def list_models(service: OllamaService = Depends(get_ollama_service)):
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        models = await service.list_models()
        return QueryResponse(
            success=True,
            data=[model.model_dump() for model in models],
            message=f"æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/models/{model_name}/status", response_model=QueryResponse)
async def get_model_status(
    model_name: str,
    service: OllamaService = Depends(get_ollama_service)
):
    """è·å–æ¨¡å‹çŠ¶æ€"""
    try:
        status = await service.get_model_status(model_name)
        return QueryResponse(
            success=True,
            data=status.model_dump(),
            message=f"æˆåŠŸè·å–æ¨¡å‹ {model_name} çš„çŠ¶æ€"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹çŠ¶æ€å¤±è´¥: {str(e)}")

@app.post("/models/{model_name}/load", response_model=QueryResponse)
async def load_model(
    model_name: str,
    service: OllamaService = Depends(get_ollama_service)
):
    """åŠ è½½æ¨¡å‹"""
    try:
        success = await service.load_model(model_name)
        return QueryResponse(
            success=success,
            data={"model_name": model_name, "loaded": success},
            message=f"æ¨¡å‹ {model_name} {'åŠ è½½æˆåŠŸ' if success else 'åŠ è½½å¤±è´¥'}"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")

@app.post("/generate", response_model=GenerationResponse)
async def generate_text(
    request: GenerationRequest,
    service: OllamaService = Depends(get_ollama_service)
):
    """ç”Ÿæˆæ–‡æœ¬"""
    try:
        result = await service.generate_text(request)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥: {str(e)}")

@app.post("/rescue-plan", response_model=QueryResponse)
async def generate_rescue_plan(
    request: RescuePlanGenerationRequest,
    service: OllamaService = Depends(get_ollama_service)
):
    """ç”Ÿæˆæ•‘æ´æ–¹æ¡ˆ"""
    try:
        rescue_plan = await service.generate_rescue_plan(request)
        return QueryResponse(
            success=True,
            data=rescue_plan.model_dump(),
            message="æ•‘æ´æ–¹æ¡ˆç”ŸæˆæˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ•‘æ´æ–¹æ¡ˆå¤±è´¥: {str(e)}")

@app.get("/cache/stats", response_model=QueryResponse)
async def get_cache_stats(service: OllamaService = Depends(get_ollama_service)):
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = await service.get_cache_stats()
        return QueryResponse(
            success=True,
            data=stats,
            message="ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.delete("/cache", response_model=QueryResponse)
async def clear_cache(service: OllamaService = Depends(get_ollama_service)):
    """æ¸…ç©ºç¼“å­˜"""
    try:
        success = await service.clear_cache()
        return QueryResponse(
            success=success,
            data={"cleared": success},
            message="ç¼“å­˜æ¸…ç©ºæˆåŠŸ" if success else "ç¼“å­˜æ¸…ç©ºå¤±è´¥"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}")

# å¯åŠ¨å’Œå…³é—­äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logger.info("OllamaæœåŠ¡å¯åŠ¨ä¸­...")
    try:
        is_healthy = await ollama_service.test_connection()
        if is_healthy:
            logger.info("OllamaæœåŠ¡å¯åŠ¨æˆåŠŸ")
        else:
            logger.warning("OllamaæœåŠ¡å¯åŠ¨ï¼Œä½†è¿æ¥å¼‚å¸¸")
    except Exception as e:
        logger.error(f"OllamaæœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    logger.info("OllamaæœåŠ¡å…³é—­ä¸­...")
    logger.info("OllamaæœåŠ¡å·²å…³é—­")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)
